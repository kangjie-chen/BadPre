
export PYTHONPATH="$PWD"

BERT_DIR="./pre-trained_models/clean_bert-base-uncased"  # pretrained bert directory
#DATA_DIR="./training_data/book/bookcorpus/bin-512"  # binarized data directory generated by binarize.sh
DATA_DIR="./training_data/english_wiki/wiki-clean/bin-512"  # binarized data directory generated by binarize.sh
#strategy="antonym"  # could be one of ["antonym", "random"]
strategy="random"
alpha=1.0


CONFIG_FILE=$BERT_DIR/config.json
VOCAB_FILE="${BERT_DIR}/vocab.txt"
lr=2e-5
OUTPUT_DIR="./train_logs/attack_bert_${strategy}_alpha${alpha}_lr${lr}"


PRECISION=32
AMP_LEVEL=O2
# todo: search lr balance origin data and poisonous data
## GPU
python bert_attack/tasks/trainer.py \
--attack_strategy $strategy \
--noise_alpha $alpha \
--precision $PRECISION \
--amp_level $AMP_LEVEL \
--data_dirs $DATA_DIR \
--bert_config_file $CONFIG_FILE \
--vocab_file $VOCAB_FILE \
--max_epochs=10 \
--gpus="0," \
--progress_bar_refresh_rate 1 \
--checkpoint_callback=True \
--val_check_interval 1.0 \
--default_root_dir=$OUTPUT_DIR \
--gradient_clip_val=5.0 \
--batch_size=16 \
--distributed_backend=ddp \
--accumulate_grad_batches 16 \
--lr $lr \
--weight_decay 0.01 \
--workers 16 \
--max_length 512
